{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user-1\\anaconda3\\envs\\tfod\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user-1\\anaconda3\\envs\\tfod\\lib\\site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: click in c:\\users\\user-1\\anaconda3\\envs\\tfod\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\user-1\\anaconda3\\envs\\tfod\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user-1\\anaconda3\\envs\\tfod\\lib\\site-packages (from nltk) (4.59.0)\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "!pip install nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>BULLET_POINTS</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>PRODUCT_TYPE_ID</th>\n",
       "      <th>PRODUCT_LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1925202</td>\n",
       "      <td>ArtzFolio Tulip Flowers Blackout Curtain for D...</td>\n",
       "      <td>[LUXURIOUS &amp; APPEALING: Beautiful custom-made ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1650</td>\n",
       "      <td>2125.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2673191</td>\n",
       "      <td>Marks &amp; Spencer Girls' Pyjama Sets T86_2561C_N...</td>\n",
       "      <td>[Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2755</td>\n",
       "      <td>393.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2765088</td>\n",
       "      <td>PRIKNIK Horn Red Electric Air Horn Compressor ...</td>\n",
       "      <td>[Loud Dual Tone Trumpet Horn, Compatible With ...</td>\n",
       "      <td>Specifications: Color: Red, Material: Aluminiu...</td>\n",
       "      <td>7537</td>\n",
       "      <td>748.031495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1594019</td>\n",
       "      <td>ALISHAH Women's Cotton Ankle Length Leggings C...</td>\n",
       "      <td>[Made By 95%cotton and 5% Lycra which gives yo...</td>\n",
       "      <td>AISHAH Women's Lycra Cotton Ankel Leggings. Br...</td>\n",
       "      <td>2996</td>\n",
       "      <td>787.401574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>283658</td>\n",
       "      <td>The United Empire Loyalists: A Chronicle of th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6112</td>\n",
       "      <td>598.424000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PRODUCT_ID                                              TITLE  \\\n",
       "0     1925202  ArtzFolio Tulip Flowers Blackout Curtain for D...   \n",
       "1     2673191  Marks & Spencer Girls' Pyjama Sets T86_2561C_N...   \n",
       "2     2765088  PRIKNIK Horn Red Electric Air Horn Compressor ...   \n",
       "3     1594019  ALISHAH Women's Cotton Ankle Length Leggings C...   \n",
       "4      283658  The United Empire Loyalists: A Chronicle of th...   \n",
       "\n",
       "                                       BULLET_POINTS  \\\n",
       "0  [LUXURIOUS & APPEALING: Beautiful custom-made ...   \n",
       "1  [Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...   \n",
       "2  [Loud Dual Tone Trumpet Horn, Compatible With ...   \n",
       "3  [Made By 95%cotton and 5% Lycra which gives yo...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         DESCRIPTION  PRODUCT_TYPE_ID  \\\n",
       "0                                                NaN             1650   \n",
       "1                                                NaN             2755   \n",
       "2  Specifications: Color: Red, Material: Aluminiu...             7537   \n",
       "3  AISHAH Women's Lycra Cotton Ankel Leggings. Br...             2996   \n",
       "4                                                NaN             6112   \n",
       "\n",
       "   PRODUCT_LENGTH  \n",
       "0     2125.980000  \n",
       "1      393.700000  \n",
       "2      748.031495  \n",
       "3      787.401574  \n",
       "4      598.424000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Reading the dataset\n",
    "train = pd.read_csv(r'C:\\Users\\User-1\\Downloads\\Amazon ML Challenge\\train.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\User-1\\Downloads\\Amazon ML Challenge\\test.csv')\n",
    "\n",
    "# To see first 5 rows of dataset\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['TITLE' ,'BULLET_POINTS', 'PRODUCT_LENGTH']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna('@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['x'] = train['TITLE'] + train['BULLET_POINTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PRODUCT_ID              0\n",
       "TITLE                   5\n",
       "BULLET_POINTS      275922\n",
       "DESCRIPTION        380001\n",
       "PRODUCT_TYPE_ID         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['TITLE' ,'BULLET_POINTS']]\n",
    "test = test.fillna('A')\n",
    "test['x'] = test['TITLE'] + test['BULLET_POINTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['x'] = train['x'].apply(lambda x : re.sub('[^a-zA-Z#\\s]*','',x))\n",
    "test['x'] = test['x'].apply(lambda x : re.sub('[^a-zA-Z#\\s]*','',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Removing Short Words\n",
    "\n",
    "We have to be a little careful here in selecting the length of the words which we want to remove. So, I have decided to remove all the words having length 3 or less. For example, terms like “hmm”, “oh” are of very little use. It is better to get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['x'] = train['x'].apply(lambda x : ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['x'] = test['x'].apply(lambda x : ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Text Normalization\n",
    "\n",
    "Here we will use nltk’s PorterStemmer() function to normalize the tweets. But before that we will have to tokenize the tweets. Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above given code takes too much time to run and as an data analyst we cant be happy with this lines of code since we are going to work with very huge dataset aso i have wriiten alternative code for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['x'] = train['x'].apply(lambda x : x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['x'] = train['x'].apply(lambda y :  [lemmatizer.lemmatize(w) for w in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['x'] = test['x'].apply(lambda x : x.split())\n",
    "test['x'] = test['x'].apply(lambda y :  [lemmatizer.lemmatize(w) for w in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets sttitch this token back to retreive our cleaned tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['x'] = train['x'].apply(lambda y : ' '.join(w for w in y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['x'] = test['x'].apply(lambda y : ' '.join(w for w in y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doing', 'those', 'above', 'mustn', 'did', 'when', 'both', 'about', \"couldn't\", 'he', \"she's\", 'just', 'each', 'aren', 'ain', 'they', 'than', 'himself', 'hadn', 'i', 'to', 'won', \"wouldn't\", 'his', 'or', 'any', \"aren't\", 'hers', 'most', 're', 'can', 'him', 'were', 'only', 'ourselves', 'very', 'why', 'yourself', 'down', 'here', 'for', 'nor', 'below', 'o', 'hasn', \"mustn't\", 'not', 'which', 'same', 'between', 'how', 'but', \"didn't\", 'our', 'the', 'couldn', 'into', 'further', \"isn't\", 'their', 'itself', 'them', 've', 'over', 'me', 'was', \"needn't\", 'at', 'there', 'she', 'once', \"should've\", \"hasn't\", 'with', 'does', 'y', 'up', 'my', 'in', 'needn', 'm', 'where', \"you've\", 'shouldn', 'from', 'its', 'wouldn', 'such', 'other', \"haven't\", \"you're\", 'through', 'have', 'it', \"you'll\", 'that', 'haven', 'out', 'because', 'yours', 's', \"weren't\", 'so', \"it's\", 'doesn', 'on', 'then', 'a', 'all', 'more', 'no', 'being', 'don', 'ma', 'am', 'during', 'too', 'you', \"don't\", \"won't\", 'we', 'theirs', 'having', 'be', \"doesn't\", 'while', 'few', 'had', 'herself', 'has', 'are', 'an', 'will', 'is', 'again', \"shouldn't\", 'ours', 'now', 'mightn', 'yourselves', 'your', 'what', 'this', 'before', 'been', 'didn', 'some', \"hadn't\", 'wasn', \"that'll\", \"you'd\", 'should', 'these', 'myself', 'and', 'against', 'until', 'll', 'as', 'who', 'do', \"mightn't\", 'after', 'if', 'by', 'themselves', 't', 'shan', 'own', 'd', 'weren', 'off', 'isn', 'whom', \"wasn't\", 'her', 'under', 'of', \"shan't\"}\n"
     ]
    }
   ],
   "source": [
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['x'] = train['x'].apply(lambda x : ' '.join([w for w in x.split() if not w in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['x'] = test['x'].apply(lambda x : ' '.join([w for w in x.split() if not w in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df = 0.9 , min_df = 2 , stop_words = 'english' , max_features = 1000)\n",
    "df_train = tfidf_vectorizer.fit_transform(train['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = tfidf_vectorizer.fit_transform(test['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('amazon_train.csv')\n",
    "test.to_csv('amazon_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2249698, 1000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[['PRODUCT_LENGTH']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr.fit(df_train , y_train)\n",
    "pred = dtr.predict(df_test)\n",
    "# predicted = dtr.predict(df_train)\n",
    "pred = pd.DataFrame(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "(1-metrics.mean_absolute_percentage_error(y_train,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to_csv(r'C:\\Users\\User-1\\Downloads\\Amazon ML Challenge\\submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD - 2 Using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "\n",
    "onehot_repr=[one_hot(words,5000)for words in df['nice_tweet']] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "embedded_doc = pad_sequences(onehot_repr , padding = 'pre' , maxlen = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 50)            250000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 310,501\n",
      "Trainable params: 310,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout , LSTM , Bidirectional , Embedding\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(5000 , 50 ,input_length=20))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999/999 [==============================] - 25s 22ms/step - loss: 0.2252 - accuracy: 0.9343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ecd0b3da48>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lstm = np.array(embedded_doc)\n",
    "y_lstm = df['label']\n",
    "model.fit(X_lstm , y_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User-1\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "a = model.predict_classes(X_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31962"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### So while using lstm training accuracy is around 95 % whereas while using concepts of nlp like vectorizer accuracy is just 50 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 20)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(embedded_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
